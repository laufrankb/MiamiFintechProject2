{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10dfe920-208d-426a-b47d-509fd79aaeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries needed, read the dataset\n",
    "\n",
    "import nltk, re, pprint, string\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "string.punctuation = string.punctuation + '“' + '”' +'-' + '’' + '‘' + '—' + '(' + ')'\n",
    "string.punctuation = string.punctuation.replace('.', '')\n",
    "file = open('hiphoprap_text.txt', encoding = 'utf8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cd7a2fd-1c5a-4668-9902-78dfa11d150a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess data\n",
    "\n",
    "file_nl_removed = \"\"\n",
    "for line in file:\n",
    "  line_nl_removed = line.replace(\"\\n\", \" \")           #removes newlines\n",
    "  file_nl_removed += line_nl_removed\n",
    "\n",
    "file_p = \"\".join([char for char in file_nl_removed if char not in string.punctuation])   #removes all special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9cb124c-564a-4d4f-9c82-fa243dc0ae6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/moniqueferguson/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Instantiate the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# Define function to clean up text\n",
    "def process_text(article, sw_custom):\n",
    "\n",
    "    # Define a set of stopwords using `stopwords.words()`\n",
    "    sw = set(stopwords.words('english'))\n",
    "\n",
    "    # Define the regex parameters\n",
    "    regex = re.compile(\"[^a-zA-Z ]\")\n",
    "    \n",
    "    # Apply regex parameters to article\n",
    "    re_clean = regex.sub('', article)\n",
    "    \n",
    "    # Apply `word_tokenize` to the regex scrubbed text\n",
    "    words = word_tokenize(re_clean)\n",
    "    \n",
    "    # Lemmatize each word from a list of words\n",
    "    lem = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    # Create list of lower-case words that are not in the stopword set\n",
    "    output = [word.lower() for word in lem if word.lower() not in sw.union(sw_custom)]\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da3b69ff-854f-4d1c-bc77-25241816dafb",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4242737290.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/vh/zjx4gqx97hg9l5h47s2zg86c0000gn/T/ipykernel_16568/4242737290.py\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    lyrics_tokenized\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Create a list of stopwords\n",
    "sw_add_ons = ['', 'the', 'a', 'at', 'for', 'above', 'on', 'is', 'all', 'ai', 'nt', 'wan', 'na', 'uh', 'ay', 'ta', 'Cashen', 'vi', 'två', 'D-D-Daytrip', 'よ！', 'じゃぁ 行こうぞ！', '-', 'D', 'sha', 'ba', 'RGF', 'rgf', 'ジャンケンポン！', '(', ')', 'dddaytrip',  'nigga', 'niggas', 'shit', 'fuck', 'bitch', 'fucked', 'pussy', 'hoe', 'motherfucker', 'bitches']\n",
    "    \n",
    "# Run the text_processor function for lyrics dataframe\n",
    "lyrics_tokenized = [process_text(text, sw_custom) for text in file_p\n",
    "lyrics_tokenized\n",
    "                    \n",
    "# Create a new tokens column for dataframe\n",
    "# lyrics_df['Tokens'] = lyrics_tokenized\n",
    "# lyrics_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a89a53c8-0ce7-44b9-8085-5bec64d8784b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of sentences is 8\n",
      "The number of tokens is 41657\n",
      "The average number of tokens per sentence is 5207\n",
      "The number of unique tokens are 6174\n"
     ]
    }
   ],
   "source": [
    "sents = nltk.sent_tokenize(file_p)\n",
    "print(\"The number of sentences is\", len(sents)) #prints the number of sentences\n",
    "\n",
    "words = nltk.word_tokenize(file_p)\n",
    "print(\"The number of tokens is\", len(words)) #prints the number of tokens\n",
    "\n",
    "average_tokens = round(len(words)/len(sents))\n",
    "print(\"The average number of tokens per sentence is\", average_tokens) #prints the average number of tokens per sentence\n",
    "\n",
    "unique_tokens = set(words)\n",
    "print(\"The number of unique tokens are\", len(unique_tokens)) #prints the number of unique tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e49a539-0e19-478a-81f8-d103578b8ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common n-grams without stopword removal and without add-1 smoothing: \n",
      "\n",
      "Most common bigrams:  [(('i', 'got'), 129), (('i', 'dont'), 106), (('like', 'a'), 87), (('yeah', 'yeah'), 86), (('i', 'aint'), 64)]\n",
      "\n",
      "Most common trigrams:  [(('capitão', 'de', 'areia'), 51), (('low', 'low', 'low'), 48), (('too', 'sexy', 'for'), 42), (('é', 'capitão', 'de'), 39), (('yeah', 'yeah', 'yeah'), 35)]\n",
      "\n",
      "Most common fourgrams:  [(('low', 'low', 'low', 'low'), 40), (('é', 'capitão', 'de', 'areia'), 39), (('i', 'want', 'is', 'you'), 29), (('thats', 'that', 'shit', 'i'), 24), (('that', 'shit', 'i', 'dont'), 24)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "unigram=[]\n",
    "bigram=[]\n",
    "trigram=[]\n",
    "fourgram=[]\n",
    "tokenized_text = []\n",
    "\n",
    "for sentence in sents:\n",
    "    sentence = sentence.lower()\n",
    "    sequence = word_tokenize(sentence) \n",
    "    for word in sequence:\n",
    "        if (word =='.'):\n",
    "            sequence.remove(word) \n",
    "        else:\n",
    "            unigram.append(word)\n",
    "    tokenized_text.append(sequence) \n",
    "    bigram.extend(list(ngrams(sequence, 2)))              #unigram, bigram, trigram, and fourgram models are created\n",
    "    trigram.extend(list(ngrams(sequence, 3)))\n",
    "    fourgram.extend(list(ngrams(sequence, 4)))\n",
    "\n",
    "def removal(x):                                    #removes ngrams containing only stopwords\n",
    "    y = []\n",
    "    for pair in x:\n",
    "        count = 0\n",
    "        for word in pair:\n",
    "            if word in stop_words:\n",
    "                count = count or 0\n",
    "            else:\n",
    "                count = count or 1\n",
    "        if (count==1):\n",
    "            y.append(pair)\n",
    "    return(y)\n",
    "\n",
    "bigram = removal(bigram)\n",
    "trigram = removal(trigram)             \n",
    "fourgram = removal(fourgram)\n",
    "\n",
    "freq_bi = nltk.FreqDist(bigram)\n",
    "freq_tri = nltk.FreqDist(trigram)\n",
    "freq_four = nltk.FreqDist(fourgram)\n",
    "\n",
    "print(\"Most common n-grams without stopword removal and without add-1 smoothing: \\n\")\n",
    "print (\"Most common bigrams: \", freq_bi.most_common(5))      #prints most common n-grams without add-1 smoothing and without stopword removal.\n",
    "print (\"\\nMost common trigrams: \", freq_tri.most_common(5))\n",
    "print (\"\\nMost common fourgrams: \", freq_four.most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e7dd8e5-584c-4ce3-9805-e09f21adab41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common n-grams with stopword removal and without add-1 smoothing: \n",
      "\n",
      "Most common unigrams:  [('im', 461), ('like', 437), ('yeah', 358), ('got', 308), ('dont', 245), ('know', 215), ('go', 174), ('cant', 174), ('get', 159), ('shit', 157)]\n",
      "\n",
      "Most common bigrams:  [(('yeah', 'yeah'), 87), (('wan', 'na'), 63), (('low', 'low'), 57), (('capitão', 'de'), 51), (('de', 'areia'), 51), (('feel', 'like'), 39), (('é', 'capitão'), 39), (('got', 'ta'), 38), (('dont', 'know'), 34), (('dont', 'like'), 33)]\n"
     ]
    }
   ],
   "source": [
    "#stopwords = code for downloading stop words through nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#prints top 10 unigrams, bigrams after removing stopwords\n",
    "\n",
    "print(\"Most common n-grams with stopword removal and without add-1 smoothing: \\n\")\n",
    "unigram_sw_removed = [p for p in unigram if p not in stop_words]\n",
    "fdist = nltk.FreqDist(unigram_sw_removed)\n",
    "print(\"Most common unigrams: \", fdist.most_common(10))\n",
    "\n",
    "bigram_sw_removed = []\n",
    "bigram_sw_removed.extend(list(ngrams(unigram_sw_removed, 2)))\n",
    "fdist = nltk.FreqDist(bigram_sw_removed)\n",
    "print(\"\\nMost common bigrams: \", fdist.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bdea3c0-9335-4811-8f19-df9b9dd13057",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add-1 smoothing is performed here:\n",
    "            \n",
    "ngrams_all = {1:[], 2:[], 3:[], 4:[]}\n",
    "for i in range(4):\n",
    "    for each in tokenized_text:\n",
    "        for j in ngrams(each, i+1):\n",
    "            ngrams_all[i+1].append(j);\n",
    "\n",
    "ngrams_voc = {1:set([]), 2:set([]), 3:set([]), 4:set([])}\n",
    "\n",
    "for i in range(4):\n",
    "    for gram in ngrams_all[i+1]:\n",
    "        if gram not in ngrams_voc[i+1]:\n",
    "            ngrams_voc[i+1].add(gram)\n",
    "\n",
    "total_ngrams = {1:-1, 2:-1, 3:-1, 4:-1}\n",
    "total_voc = {1:-1, 2:-1, 3:-1, 4:-1}\n",
    "for i in range(4):\n",
    "    total_ngrams[i+1] = len(ngrams_all[i+1])\n",
    "    total_voc[i+1] = len(ngrams_voc[i+1])                       \n",
    "    \n",
    "ngrams_prob = {1:[], 2:[], 3:[], 4:[]}\n",
    "for i in range(4):\n",
    "    for ngram in ngrams_voc[i+1]:\n",
    "        tlist = [ngram]\n",
    "        tlist.append(ngrams_all[i+1].count(ngram))\n",
    "        ngrams_prob[i+1].append(tlist)\n",
    "    \n",
    "for i in range(4):\n",
    "    for ngram in ngrams_prob[i+1]:\n",
    "        ngram[-1] = (ngram[-1]+1)/(total_ngrams[i+1] + total_voc[i+1])             #add-1 smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94d927f9-c7f1-4269-8b8c-726aa46e5479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common n-grams without stopword removal and with add-1 smoothing: \n",
      "\n",
      "Most common unigrams:  [[('i',), 0.030445412990326354], [('the',), 0.0235569256936324], [('you',), 0.023408100350802593], [('it',), 0.01688104602955246], [('a',), 0.016817263739768256], [('me',), 0.013840756883172106], [('my',), 0.01233124269161263], [('and',), 0.010928032316360158], [('to',), 0.010630381630700542], [('on',), 0.010481556287870734]]\n",
      "\n",
      "Most common bigrams:  [[('in', 'the'), 0.002301011148561058], [('i', 'got'), 0.0021065595022037855], [('i', 'dont'), 0.0017338605133523464], [('on', 'my'), 0.0014259787399533315], [('like', 'a'), 0.0014259787399533315], [('yeah', 'yeah'), 0.0014097744360902255], [('to', 'the'), 0.0010856883588281048], [('i', 'aint'), 0.0010532797511018928], [('you', 'know'), 0.0010370754472387865], [('and', 'i'), 0.0010370754472387865]]\n",
      "\n",
      "Most common trigrams:  [[('capitão', 'de', 'areia'), 0.0007502741386275754], [('low', 'low', 'low'), 0.0007069890921682923], [('too', 'sexy', 'for'), 0.0006204189992497259], [('é', 'capitão', 'de'), 0.0005771339527904426], [('yeah', 'yeah', 'yeah'), 0.0005194205575113984], [('what', 'what', 'what'), 0.0005049922086916373], [('want', 'is', 'you'), 0.000432850464592832], [('i', 'dont', 'like'), 0.000432850464592832], [('i', 'want', 'is'), 0.000432850464592832], [('shit', 'i', 'dont'), 0.0003751370693137877]]\n",
      "\n",
      "Most common fourgrams:  [[('low', 'low', 'low', 'low'), 0.000576150192518479], [('é', 'capitão', 'de', 'areia'), 0.000562097748798516], [('i', 'want', 'is', 'you'), 0.00042157331159888706], [('what', 'what', 'what', 'what'), 0.00042157331159888706], [('thats', 'that', 'shit', 'i'), 0.00035131109299907257], [('that', 'shit', 'i', 'dont'), 0.00035131109299907257], [('shit', 'i', 'dont', 'like'), 0.00035131109299907257], [('na', 'bahia', 'é', 'capitão'), 0.00033725864927910965], [('out', 'poke', 'it', 'out'), 0.00033725864927910965], [('bahia', 'é', 'capitão', 'de'), 0.00033725864927910965]]\n"
     ]
    }
   ],
   "source": [
    "#Prints top 10 unigrams, bigrams, trigrams, fourgrams after smoothing\n",
    "\n",
    "print(\"Most common n-grams without stopword removal and with add-1 smoothing: \\n\")\n",
    "for i in range(4):\n",
    "    ngrams_prob[i+1] = sorted(ngrams_prob[i+1], key = lambda x:x[1], reverse = True)\n",
    "    \n",
    "print (\"Most common unigrams: \", str(ngrams_prob[1][:10]))\n",
    "print (\"\\nMost common bigrams: \", str(ngrams_prob[2][:10]))\n",
    "print (\"\\nMost common trigrams: \", str(ngrams_prob[3][:10]))\n",
    "print (\"\\nMost common fourgrams: \", str(ngrams_prob[4][:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d2e760f-f969-48f7-9c5a-d1fc73cd6638",
   "metadata": {},
   "outputs": [],
   "source": [
    "str1 = 'after that alice said the'\n",
    "str2 = 'alice felt so desperate that she was'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27c8bf2b-a93b-423e-9634-c46348c3d352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String 1:  {1: ('the',), 2: ('said', 'the'), 3: ('alice', 'said', 'the')} \n",
      "String 2:  {1: ('was',), 2: ('she', 'was'), 3: ('that', 'she', 'was')}\n"
     ]
    }
   ],
   "source": [
    "#smoothed models without stopwords removed are used\n",
    "\n",
    "token_1 = word_tokenize(str1)\n",
    "token_2 = word_tokenize(str2)\n",
    "\n",
    "ngram_1 = {1:[], 2:[], 3:[]}                  #to store n-grams formed from the strings\n",
    "ngram_2 = {1:[], 2:[], 3:[]}\n",
    "for i in range(3):\n",
    "    ngram_1[i+1] = list(ngrams(token_1, i+1))[-1]\n",
    "    ngram_2[i+1] = list(ngrams(token_2, i+1))[-1]\n",
    "\n",
    "print(\"String 1: \", ngram_1,\"\\nString 2: \",ngram_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ebff516-c07d-4085-a747-7c9cded4a5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    ngrams_prob[i+1] = sorted(ngrams_prob[i+1], key = lambda x:x[1], reverse = True)\n",
    "    \n",
    "pred_1 = {1:[], 2:[], 3:[]}\n",
    "for i in range(3):\n",
    "    count = 0\n",
    "    for each in ngrams_prob[i+2]:\n",
    "        if each[0][:-1] == ngram_1[i+1]:       #to find predictions based on highest probability of n-grams                   \n",
    "            count +=1\n",
    "            pred_1[i+1].append(each[0][-1])\n",
    "            if count ==5:\n",
    "                break\n",
    "    if count<5:\n",
    "        while(count!=5):\n",
    "            pred_1[i+1].append(\"NOT FOUND\")           #if no word prediction is found, replace with NOT FOUND\n",
    "            count +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6619b379-2ad7-4ae4-9cb5-c029492d259d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    ngrams_prob[i+1] = sorted(ngrams_prob[i+1], key = lambda x:x[1], reverse = True)\n",
    "    \n",
    "pred_2 = {1:[], 2:[], 3:[]}\n",
    "for i in range(3):\n",
    "    count = 0\n",
    "    for each in ngrams_prob[i+2]:\n",
    "        if each[0][:-1] == ngram_2[i+1]:\n",
    "            count +=1\n",
    "            pred_2[i+1].append(each[0][-1])\n",
    "            if count ==5:\n",
    "                break\n",
    "    if count<5:\n",
    "        while(count!=5):\n",
    "            pred_2[i+1].append(\"\\0\")\n",
    "            count +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa977429-6f85-4a3b-9e28-fb5fb0353747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next word predictions for the strings using the probability models of bigrams, trigrams, and fourgrams\n",
      "\n",
      "String 1 - after that alice said the-\n",
      "\n",
      "Bigram model predictions: ['world', 'floor', 'air', 'ground', 'night']\n",
      "Trigram model predictions: ['NOT FOUND', 'NOT FOUND', 'NOT FOUND', 'NOT FOUND', 'NOT FOUND']\n",
      "Fourgram model predictions: ['NOT FOUND', 'NOT FOUND', 'NOT FOUND', 'NOT FOUND', 'NOT FOUND']\n",
      "\n",
      "String 2 - alice felt so desperate that she was-\n",
      "\n",
      "Bigram model predictions: ['saying', 'gettin', 'lookin', 'just', 'a']\n",
      "Trigram model predictions: ['a', 'all', 'pregnant', 'flexible', 'worth']\n",
      "Fourgram model predictions: ['\\x00', '\\x00', '\\x00', '\\x00', '\\x00']\n"
     ]
    }
   ],
   "source": [
    "print(\"Next word predictions for the strings using the probability models of bigrams, trigrams, and fourgrams\\n\")\n",
    "print(\"String 1 - after that alice said the-\\n\")\n",
    "print(\"Bigram model predictions: {}\\nTrigram model predictions: {}\\nFourgram model predictions: {}\\n\" .format(pred_1[1], pred_1[2], pred_1[3]))\n",
    "print(\"String 2 - alice felt so desperate that she was-\\n\")\n",
    "print(\"Bigram model predictions: {}\\nTrigram model predictions: {}\\nFourgram model predictions: {}\" .format(pred_2[1], pred_2[2], pred_2[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f227a0-0ff9-460d-b7f6-9ab6a97f0a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
